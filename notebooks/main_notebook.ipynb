{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from io import StringIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from gpt4all import GPT4All\n",
    "from langchain_community.llms import GPT4All as LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ],
   "id": "423e324180126f72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "model_directory = os.path.join(project_root, 'model')"
   ],
   "id": "c2c9af3d127baf72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def download_pdf_from_url(url, save_path):\n",
    "    \"\"\"\n",
    "    Download a PDF file from the given URL and save it to the specified path.\n",
    "\n",
    "    :param url: The URL of the PDF file.\n",
    "    :param save_path: The path where the PDF file should be saved.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as out_file:\n",
    "        out_file.write(response.content)"
   ],
   "id": "187c7c540506991a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "\n",
    "    :param pdf_path: The path to the PDF file.\n",
    "    :return: The extracted text from the PDF file.\n",
    "    \"\"\"\n",
    "    resource_manager = PDFResourceManager()\n",
    "    fake_file_handle = StringIO()\n",
    "    converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "        text = fake_file_handle.getvalue()\n",
    "\n",
    "    converter.close()\n",
    "    fake_file_handle.close()\n",
    "    os.remove(pdf_path)\n",
    "\n",
    "    if text:\n",
    "        return text"
   ],
   "id": "613c685a5da5880f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by removing extra whitespaces and splitting it into sentences.\n",
    "\n",
    "    :param text: The text to be processed.\n",
    "    :return: A list of sentences derived from the text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    sentences = [sentence for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n"
   ],
   "id": "8ec39939d8c9b291",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def embed_sentences(sentences, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Embed sentences using the BERT model.\n",
    "\n",
    "    :param sentences: A list of sentences to be embedded.\n",
    "    :param use_gpu: A boolean indicating whether to use GPU for embedding or not. Default is False.\n",
    "    :return: A tensor containing the embedded representations of the sentences.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', resume_download=False)\n",
    "    model.to(device)\n",
    "\n",
    "    tokenized_texts = [tokenizer.encode(sentence, add_special_tokens=True, max_length=512, truncation=True) for sentence in sentences]\n",
    "    max_len = max(len(sent) for sent in tokenized_texts)\n",
    "    padded_tokenized_texts = [sent + [tokenizer.pad_token_id] * (max_len - len(sent)) for sent in tokenized_texts]\n",
    "\n",
    "    indexed_tokens = torch.tensor(padded_tokenized_texts).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=indexed_tokens)\n",
    "        encoded_layers = outputs.last_hidden_state\n",
    "\n",
    "    embeddings = encoded_layers[:, 0, :]\n",
    "    return embeddings"
   ],
   "id": "c637ee812230077f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Create a FAISS index with the given embeddings.\n",
    "\n",
    "    :param embeddings: The embeddings to be indexed.\n",
    "    :return: The created FAISS index.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexFlatL2(embeddings.size(-1))\n",
    "    index.add(embeddings.cpu().numpy())\n",
    "    return index"
   ],
   "id": "341dfe74c70297e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def download_model(model_url, model_directory, model_filename):\n",
    "    \"\"\"\n",
    "    Download a model from the given URL and save it to the specified directory.\n",
    "\n",
    "    :param model_url: The URL of the model to download.\n",
    "    :param model_directory: The directory to save the downloaded model.\n",
    "    :param model_filename: The filename to save the downloaded model as.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(model_directory, model_filename)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Model found\")\n",
    "    else:\n",
    "        print(\"Model not found. Starting download...\")\n",
    "        os.makedirs(model_directory, exist_ok=True)\n",
    "        response = requests.get(model_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Model successfully downloaded and saved.\")\n",
    "        else:\n",
    "            print(\"Failed to download model.\")"
   ],
   "id": "4c172203b8114ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_langchain(question, context, model_path):\n",
    "    \"\"\"\n",
    "    Create a LangChain LLMChain to generate an answer based on the question and context.\n",
    "\n",
    "    :param question: The question to be answered.\n",
    "    :param context: The context related to the question.\n",
    "    :param model_path: The path to the model file.\n",
    "    :return: The generated answer.\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "    llm = LLM(model=model_path)\n",
    "    chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    answer = chain.run(context=context, question=question)\n",
    "    return answer"
   ],
   "id": "b7ca86640f337125",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_similar_sentences_in_pdf(question, pdf_text, index, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Find the most similar sentence in a PDF based on a given question.\n",
    "\n",
    "    :param question: The question to compare sentences against.\n",
    "    :param pdf_text: The text content of the PDF.\n",
    "    :param index: The FAISS index used for similarity search.\n",
    "    :param use_gpu: A boolean indicating whether to use GPU for embeddings. Default is False.\n",
    "    :return: The most similar sentence to the question in the PDF.\n",
    "    \"\"\"\n",
    "    question_embedding = embed_sentences([question], use_gpu)\n",
    "    D, I = index.search(question_embedding.cpu().numpy(), k=1)\n",
    "    most_similar_index = I[0][0]\n",
    "    most_similar_sentence = pdf_text[most_similar_index]\n",
    "    return most_similar_sentence"
   ],
   "id": "e3b4d2f95df4951a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main(url, question, device_type):\n",
    "    \"\"\"\n",
    "    Main function to download a PDF, extract text, create embeddings, search for similar sentences,\n",
    "    and generate an answer using LangChain.\n",
    "\n",
    "    :param url: The URL of the PDF file.\n",
    "    :param question: The question to be answered.\n",
    "    :param device_type: The type of device to use for embeddings (either 'cpu' or 'gpu').\n",
    "    :return: The generated answer.\n",
    "    \"\"\"\n",
    "    if device_type == \"cpu\":\n",
    "        use_gpu = False\n",
    "    elif device_type == \"gpu\":\n",
    "        use_gpu = True\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for device_type flag. Please provide either 'cpu' or 'gpu'.\")\n",
    "\n",
    "    model_url = \"https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf\"\n",
    "    model_directory = \"../\"  # Relative path to the root directory\n",
    "    model_filename = \"orca-2-7b.Q4_0.gguf\"\n",
    "    model_path = os.path.join(model_directory, model_filename)\n",
    "    pdf_save_path = \"temp.pdf\"\n",
    "\n",
    "    download_pdf_from_url(url, pdf_save_path)\n",
    "    download_model(model_url, model_directory, model_filename)\n",
    "    pdf_text = extract_text_from_pdf(pdf_save_path)\n",
    "    pdf_preprocessed_text = preprocess_text(pdf_text)\n",
    "    embeddings = embed_sentences(pdf_preprocessed_text, use_gpu)\n",
    "    index = create_faiss_index(embeddings)\n",
    "    context = find_similar_sentences_in_pdf(question, pdf_preprocessed_text, index, use_gpu)\n",
    "    answer = create_langchain(question, context, model_path)\n",
    "\n",
    "    return answer"
   ],
   "id": "59a1729b4993680b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = \"https://media.pragprog.com/titles/ktuk/excerpts.pdf\"\n",
    "question = \"what is a widget?\"\n",
    "device_type = \"cpu\"\n",
    "\n",
    "answer = main(url, question, device_type)\n",
    "print(\"Answer:\", answer)"
   ],
   "id": "dce574758dc8e9ae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
